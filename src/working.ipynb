{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bigger-reviewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, vstack, hstack\n",
    "from scipy.optimize import LinearConstraint, Bounds, minimize\n",
    "from typing import List, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-column",
   "metadata": {},
   "source": [
    "## Markov Models, Sequential Data\n",
    "\n",
    "\n",
    "The likelihood of a sequence $(x_1, x_2,\\ldots,x_N)$ can be defined as a markov model of $M^{\\mathrm{th}}$ order by\n",
    "\n",
    "\\begin{equation}\n",
    "p(x_1,\\ldots,x_N) = p(x_1) p(x_2|x_1)\\ldots p(x_{M}|x_{M-1},\\ldots x_1)\\prod_{n=M}^N p(x_n| x_{n-1},\\ldots x_{n-M}).\n",
    "\\end{equation}\n",
    "\n",
    "The simplest order model where $M=1$ is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "p(x_1,\\ldots,x_N) = p(x_1) \\prod_{n=2}^N p(x_n | x_{n-1}).\n",
    "\\end{equation}\n",
    "\n",
    "For this model, if $x\\in\\mathcal{N}$ is discrete with $K$ possible values, we can define $p(x_n|x_{n-1})$ by the transition matrix $\\mathbf{A}: A_{ij}=p(x_n=i|x_{n-1}=j)$. This is a $K\\times K$ rank 2 tensor but because we know that\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_i p(x_n=i|x_{n-1}=j) = 1, \\forall j\n",
    "\\end{equation}\n",
    "\n",
    "there are $K-1$ free variables per row of $\\mathbf{A}$ and $K(K-1)$ free variables in total. For $M=1$,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{A} =\n",
    "\\begin{bmatrix}\n",
    "a_0 & 1-a_0 \\\\\n",
    "a_1 & 1-a_1\n",
    "\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "indonesian-mailman",
   "metadata": {
    "code_folding": [
     22,
     40,
     62,
     77,
     84,
     106,
     129,
     146,
     174,
     181,
     204,
     230,
     277,
     288,
     298,
     309,
     316,
     324,
     333,
     347
    ]
   },
   "outputs": [],
   "source": [
    "class ConditionalDistribution:\n",
    "    \"\"\"\n",
    "    Conditional probability distribution p(x_n | x_n-1, .., x_n-M) for \n",
    "    a M^th order Markox sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, K: int, M: int, random_init: bool=False):\n",
    "        if K < 2:\n",
    "            raise ValueError(f'Must have at least 2 possible categorical value.')\n",
    "        if M < 0:\n",
    "            raise ValueError('Model order must be at least 0.')\n",
    "        \n",
    "        # number of possible categorical values\n",
    "        self.K = K\n",
    "        \n",
    "        # order of Markovian dependence\n",
    "        self.M = M\n",
    "        \n",
    "        self.init_A(random=random_init)\n",
    "        \n",
    "        # number of free parameters\n",
    "        self.N = (self.K-1)*(self.K**self.M)\n",
    "        \n",
    "    def init_A(self, random: bool=False):\n",
    "        \"\"\"\n",
    "        Instantiate and set state transition probabilities\n",
    "        \"\"\"\n",
    "        if random:\n",
    "            # N(0,1)\n",
    "            self.A = np.random.normal(size=[self.K]*(self.M+1))\n",
    "            \n",
    "            # make positive\n",
    "            self.A = np.exp(self.A)\n",
    "            \n",
    "            # normalize\n",
    "            self.A /= np.tile(np.expand_dims(np.sum(self.A, axis=0), axis=0),\n",
    "                         [self.A.shape[0]]+[1]*(len(self.A.shape)-1))\n",
    "        else:\n",
    "            # uniform initial values for transition probabilities\n",
    "            self.A = np.ones([self.K]*(self.M+1))/self.K\n",
    "        \n",
    "    def prob(self, *args) -> float:\n",
    "        \"\"\"\n",
    "        Return p(args[0]|args[1],args[2],..,args[M])\n",
    "        \n",
    "        args[0] = x_n\n",
    "        args[1] = x_n-1\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "        args[M] = x_n-M\n",
    "        \n",
    "        Assume that elements of *args are integers in the range 0 to K-1\n",
    "        inclusive.\n",
    "        \"\"\"\n",
    "        if len(args) != self.M+1:\n",
    "            raise Exception('Input shape doesnt match expected shape')\n",
    "            \n",
    "        return self.A[tuple(args)]\n",
    "    \n",
    "    def log_prob(self, *args) -> float:\n",
    "        return np.log(self.prob(*args))\n",
    "            \n",
    "    def grad_log_prob(self, *args) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns the gradient of ln p(x_n|x_n-1..) with respect to A.\n",
    "        \n",
    "        A[-1,...] = 1-A[:-1,...] therefore \n",
    "        \n",
    "        grad A[:-1,...] = grad A[:-1,...] - grad A[-1,...]\n",
    "        \"\"\"\n",
    "        out = np.zeros(self.A.shape)\n",
    "        out[tuple(args)] = 1.0/self.A[tuple(args)]\n",
    "        #  shape = (K,...,K)\n",
    "        #return out\n",
    "        # shape = (K-1,...,K) -> (-1, )\n",
    "        return np.reshape(out[:-1,...] - out[-1,...], (-1, ))\n",
    "    \n",
    "    def get_params(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns 1-d array of free parameters.\n",
    "        \"\"\"\n",
    "        # p(x_n=K-1 | x_n-1 = j) = 1 - sum_k=0^K-2 p(x_n=k | x_n-1 = j)\n",
    "        return np.reshape(self.A[:-1, ...], (-1, ))\n",
    "    \n",
    "    def set_params(self, A_small: np.ndarray):\n",
    "        \"\"\"\n",
    "        Set self.A, the (K,...,K) rank M+1 tensor for conditional \n",
    "        transition probabilities from the (K-1, K, ..., K) rank M tensor\n",
    "        A_small.\n",
    "        \n",
    "        Keyword arguments\n",
    "        -----------------\n",
    "        A_small, np.ndarray, shape = [(K-1)*K^M, ]\n",
    "        \"\"\"\n",
    "        # rank M tensor, shape = (K-1, K, ...,K)\n",
    "        A = np.reshape(A_small, tuple([self.K-1]+[self.K]*self.M))\n",
    "        \n",
    "        # rank M tensor, shape = (1, K, ..., K)\n",
    "        A_ = 1.0 - np.reshape(np.sum(A, axis=0), tuple([1]+[self.K]*self.M))\n",
    "        \n",
    "        if self.M > 0:\n",
    "            # rank M tensor, shape = (K,...,K)\n",
    "            self.A = np.vstack((A, A_))\n",
    "        else:\n",
    "            self.A = np.hstack((A, A_))\n",
    "        \n",
    "    def sample(self, *args) -> int:\n",
    "        \"\"\"\n",
    "        Return a sample conditioned on the Markov blanket, *args. \n",
    "        \n",
    "        \n",
    "        For example, for a model of order self.M=2, this class models\n",
    "        the transition states p(x3|x2,x1). *args=[x2, x1] will generate\n",
    "        a sample x3 ~ p(x3|x2,x1). Note the assumed order of x2 and x1\n",
    "        in *args.\n",
    "        \n",
    "        Keyword arguments\n",
    "        -----------------\n",
    "        *args: List[float] -- for *args=[x2,x1] (self.M=2), return x3\n",
    "            such that x3 ~ p(x3|x2,x1). \n",
    "        \"\"\"\n",
    "        if len(args) != self.M:\n",
    "            raise ValueError('Arguments must be a list of values of Markov blanket.')\n",
    "            \n",
    "        # transition probabilities to state k, shape = (K, )\n",
    "        probs = self.A.T[tuple(args[::-1])]\n",
    "        \n",
    "        return np.where(np.random.multinomial(1, pvals=probs))[0][0]\n",
    "       \n",
    "    def generate_constraint_matrix(self) -> csr_matrix:\n",
    "        \"\"\"\n",
    "        Generate a matrix C such that 0 <= C self.get_params() <= 1\n",
    "        define the constraints that sum_{x_n} p(x_n | x_n-1,...) = 1\n",
    "        \n",
    "        C.shape = ((K-1)^M, N)\n",
    "        \"\"\"\n",
    "        C = np.zeros((self.K**self.M, self.N))\n",
    "        \n",
    "        for cc in range(C.shape[0]):\n",
    "            C[cc, (self.K-1)*cc:(self.K-1)*(cc+1)] = 1.0\n",
    "            \n",
    "        return csr_matrix(C)\n",
    "        \n",
    "        \n",
    "            \n",
    "class MarkovModel:\n",
    "    def __init__(self, K: int, M: int, random_init: bool=False):\n",
    "        self.K = K\n",
    "        self.M = M\n",
    "        \n",
    "        self.models = [ConditionalDistribution(K=K, M=_m, random_init=random_init)\n",
    "                       for _m in range(M+1)]\n",
    "        \n",
    "        # total number of parameters\n",
    "        self.N = sum([_m.N for _m in self.models])\n",
    "        \n",
    "    def log_prob(self, x: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Returns ln p(x0) + ln p(x1|x0) ... + ln p(x_M-1|...x0) + sum_n p(x_n | x_n-1...)\n",
    "        \"\"\"\n",
    "        if x.shape[0] < self.M+1:\n",
    "            raise ValueError(f\"Input data must have length of at least {self.M+1}\")\n",
    "        \n",
    "        if self.M > 0:\n",
    "            # ln p(x0) +...+ ln p(x_M-1 | ...x0)\n",
    "            out = sum([self.models[_m].log_prob(*x[0:_m+1][::-1]) for _m in range(self.M)])\n",
    "        else:\n",
    "            out = 0.0\n",
    "        \n",
    "        for _n in range(self.M, x.shape[0]):\n",
    "            out += self.models[self.M].log_prob(*x[_n-self.M:_n+1][::-1])\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    def get_params(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns a 1-d array of concatenated parameters over all\n",
    "        self.models, in order of increasing order.\n",
    "        \"\"\"\n",
    "        return np.concatenate(tuple(_m.get_params() for _m in self.models))\n",
    "    \n",
    "    def set_params(self, A_small: np.ndarray):\n",
    "        \"\"\"\n",
    "        Sets _m.A for _m in self.models from a concatenated list of \n",
    "        parameter values.\n",
    "        \n",
    "        Keyword arguments\n",
    "        -----------------\n",
    "        A_small: np.ndarray, shape = [Ntotal, ] -- All Ntotal free\n",
    "            parameters of all m^th order models.\n",
    "        \"\"\"\n",
    "        # number of free params in each m^th order model\n",
    "        N = [_m.N for _m in self.models]\n",
    "        \n",
    "        if sum(N) != A_small.shape[0]:\n",
    "            raise Error('Inconsistent shape of input argument. Expected [{N}, ]')\n",
    "            \n",
    "        # start and finish slice indices for each model into A_small\n",
    "        idx = [sum(N[0:i]) for i in range(self.M+1)]\n",
    "        \n",
    "        for mm in range(self.M):\n",
    "            self.models[mm].set_params(A_small[idx[mm]: idx[mm+1]])\n",
    "        self.models[-1].set_params(A_small[idx[-1]: ])\n",
    "            \n",
    "    def grad_log_prob(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns the jacobian of the log likelihood with respect to \n",
    "        model free parameters (state transition probabilities). The \n",
    "        jacobian of each m^th order model are concatenated into a 1-d\n",
    "        array in increading model order.\n",
    "        \n",
    "        Keyword arguments\n",
    "        -----------------\n",
    "        x: np.ndarray, shape = [N, ] -- categorical data of length N\n",
    "            and with self.K possible values = [0, self.K-1]\n",
    "        \"\"\"\n",
    "        \n",
    "        grad2 = np.sum([self.models[self.M].grad_log_prob(*x[_n-self.M:_n+1][::-1])\n",
    "                    for _n in range(self.M, x.shape[0])], axis=0)\n",
    "            \n",
    "        if self.M > 0:\n",
    "            grad1 = self.models[0].grad_log_prob(x[0])\n",
    "            \n",
    "            for _m in range(1, self.M):\n",
    "                grad1 = np.concatenate((grad1, self.models[_m].grad_log_prob(*x[0:_m+1][::-1]) ))\n",
    "                \n",
    "            return np.concatenate((grad1, grad2))\n",
    "        else:\n",
    "            return grad2\n",
    "\n",
    "    def sample(self, N: int, x: Union[List, np.ndarray]=[]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Return a sampled sequence of length N.\n",
    "        \n",
    "        Keyword arguments\n",
    "        -----------------\n",
    "        N: int -- The length of the sequence to return.\n",
    "        x: Union[List, np.ndarray], default = [] -- An optional seed to\n",
    "            begin sampling from.\n",
    "            \n",
    "        Examples\n",
    "        ---------\n",
    "        > from pymm import MarkovModel\n",
    "        >\n",
    "        > model = MarkovModel(2, 0)\n",
    "        > model.fit([0, 1, 0, 1, 0, 1])\n",
    "        >\n",
    "        > # sequence depends on seed values\n",
    "        > print(f'sequence 1: {model.sample(10, [0])}')\n",
    "        > print(f'sequence 2: {model.sample(10, [1])}')\n",
    "        \n",
    "        \"\"\"\n",
    "        if N<1:\n",
    "            raise ValueError('Must sample at least 1 value.')\n",
    "        \n",
    "        x = list(deepcopy(x))\n",
    "        \n",
    "        if self.M > 0:\n",
    "            for _m in range(self.M):\n",
    "                if len(x) >= N:\n",
    "                    break\n",
    "\n",
    "                if len(x)-1 < _m:\n",
    "\n",
    "                    # ConditionalDistribution.sample(*args) expects\n",
    "                    # *args = [x_{n-1}, x_{n-2},...]\n",
    "                    x.append(self.models[_m].sample(*x[:_m+1][::-1]))\n",
    "                \n",
    "        for _n in range(self.M, N):\n",
    "            if len(x)-1 < _n:\n",
    "                # ConditionalDistribution.sample(*args) expects\n",
    "                # *args = [x_{n-1}, x_{n-2},...]\n",
    "                x.append(self.models[self.M].sample(*x[_n-self.M:_n+1][::-1]))\n",
    "        \n",
    "        # shape = [N, ]\n",
    "        return np.asarray(x)\n",
    "        \n",
    "    def generate_constraint_matrix(self) -> csr_matrix:\n",
    "        \"\"\"\n",
    "        Concatenate constraint matrix for all models into a single\n",
    "        constraint matrix C such that 0 <= C * self.get_params() <= 1,\n",
    "        which corresponds to the probability normalisation contraint.\n",
    "        \"\"\"\n",
    "        # List[csr_matrix], len = self.M+1\n",
    "        constraints = [_m.generate_constraint_matrix() for _m in self.models]\n",
    "        \n",
    "        if len(constraints) < 2:\n",
    "            return constraints[0]\n",
    "        else:\n",
    "            out = self.stack(constraints[0], constraints[1])\n",
    "            if len(constraints) < 3:\n",
    "                return out\n",
    "            else:\n",
    "                for _c in constraints[2: ]:\n",
    "                    out = self.stack(out, _c)\n",
    "                    \n",
    "                return out\n",
    "    \n",
    "    def generate_constraint(self) -> LinearConstraint:\n",
    "        \"\"\"\n",
    "        Returns a scipy.optimizse.LinearConstraint instance\n",
    "        \"\"\"\n",
    "        C = self.generate_constraint_matrix()\n",
    "        \n",
    "        # avoid 1/0 probabilities\n",
    "        delta = 1e-6\n",
    "        \n",
    "        return LinearConstraint(C, np.zeros(C.shape[0])+delta, np.ones(C.shape[0])-delta)\n",
    "    \n",
    "    def generate_bounds(self) -> Bounds:\n",
    "        \"\"\"\n",
    "        Return a bounds object, constraining each transition probability\n",
    "        component to be positive.\n",
    "        \"\"\"\n",
    "        return Bounds(np.zeros(self.N), np.ones(self.N))\n",
    "    \n",
    "    def _log_prob(self, params: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Returns log likelihood as an explicit function of free\n",
    "        parameters, assuming self.x has been previously set.\n",
    "        \"\"\"\n",
    "        self.set_params(params)\n",
    "        return -self.log_prob(self.x)\n",
    "    \n",
    "    def _grad_log_prob(self, params: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns gradient of log-likelihood with respect to transition\n",
    "        state probability free parameters, assuming self.x: np.ndarray\n",
    "        has been set.\n",
    "        \"\"\"\n",
    "        self.set_params(params)\n",
    "        return -self.grad_log_prob(self.x)\n",
    "    \n",
    "    def fit(self, x: np.ndarray):\n",
    "        self.x = x\n",
    "        \n",
    "        bounds = self.generate_bounds()\n",
    "        constraints = self.generate_constraint()\n",
    "        \n",
    "        self.log = minimize(fun=self._log_prob,\n",
    "                            jac=self._grad_log_prob,\n",
    "                            x0=self.get_params(),\n",
    "                            bounds=bounds,\n",
    "                            constraints=constraints\n",
    "                            )\n",
    "\n",
    "    @classmethod\n",
    "    def stack(cls, c1: csr_matrix, c2: csr_matrix) -> csr_matrix:\n",
    "        \"\"\"\n",
    "        c1.shape = (n1, d1) -- n1 linear constraints and d1 associated\n",
    "            parameters.\n",
    "        \"\"\"\n",
    "        (n1, d1) = c1.shape\n",
    "        (n2, d2) = c2.shape\n",
    "        \n",
    "        # shape = [n1+n2, d1+d2]\n",
    "        C = csr_matrix((n1+n2, d1+d2))\n",
    "        \n",
    "        C[:n1, :d1] = c1[:, :]\n",
    "        C[n1:, d1:] = c2[:, :]\n",
    "        \n",
    "        return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "behind-sample",
   "metadata": {
    "code_folding": [
     3,
     11,
     15,
     34,
     42,
     55
    ]
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from scipy.optimize import approx_fprime\n",
    "\n",
    "def _test_getset(K: int, M: int):\n",
    "    inst = ConditionalDistribution(K=K, M=M, random_init=True)\n",
    "    \n",
    "    Aorig = deepcopy(inst.A)\n",
    "    inst.set_params(inst.get_params())\n",
    "    Arecon = deepcopy(inst.A)\n",
    "    return np.allclose(Aorig, Arecon)\n",
    "\n",
    "def test_getset():\n",
    "    out = [[_test_getset(K=_k, M=_m) for _m in range(5)] for _k in range(2, 4)]\n",
    "    assert np.all(out), 'mismatch between getter and setter'\n",
    "\n",
    "def _test_grad(model, x: np.ndarray):\n",
    "    def log_prob(params: np.ndarray):\n",
    "        # log prob as function of free model params \n",
    "        # for fixed sequence x\n",
    "        _model = deepcopy(model)\n",
    "        _model.set_params(params)\n",
    "        if isinstance(_model, ConditionalDistribution):\n",
    "            return _model.log_prob(*x)\n",
    "        else:\n",
    "            return _model.log_prob(x)\n",
    "    \n",
    "    numerical_grad = approx_fprime(model.get_params(), log_prob, 1e-8)\n",
    "    if isinstance(model, ConditionalDistribution):\n",
    "        analytical_grad = model.grad_log_prob(*x)\n",
    "    else:\n",
    "        analytical_grad = model.grad_log_prob(x)\n",
    "\n",
    "    assert np.allclose(numerical_grad, analytical_grad), f'gradient computation wrong: {analytical_grad} != {numerical_grad}'\n",
    "    \n",
    "def test_grad_ConditionalDistribution():\n",
    "    # sequence\n",
    "    x = np.asarray([1, 1], dtype=int)\n",
    "    \n",
    "    model = ConditionalDistribution(K=2, M=1, random_init=True)\n",
    "    \n",
    "    _test_grad(model, x)\n",
    "    \n",
    "def test_grad_MarkovModel():\n",
    "    \"\"\"\n",
    "    Comparae finite and analytical derivative of log likelihood with \n",
    "    respect to all model parameters.\n",
    "    \"\"\"\n",
    "    # sequence\n",
    "    x = np.asarray([1, 1, 0, 1, 0], dtype=int)\n",
    "    \n",
    "    for _m in range(4):\n",
    "        model = MarkovModel(K=2, M=1, random_init=True)\n",
    "\n",
    "        _test_grad(model, x)\n",
    "    \n",
    "def test_logprob_MarkovModel():\n",
    "    \"\"\"\n",
    "    Check cumulative log probability over models is correct\n",
    "    \"\"\"\n",
    "    def _test(M: int):\n",
    "        # p(x1,x2,x3,x4,x5)=p(x5|x4,x3)p(x4|x3,x2)p(x3|x2,x1)p(x2|x1)p(x1)\n",
    "        model = MarkovModel(K=2, M=M, random_init=True)\n",
    "\n",
    "        # sequence\n",
    "        x = np.asarray([0, 1, 0, 1, 1], dtype=int)\n",
    "\n",
    "        if M==0:\n",
    "            true = sum([model.models[0].log_prob(_x) for _x in x])\n",
    "        elif M==1:\n",
    "            # ln p(x1)\n",
    "            true = model.models[0].log_prob(x[0])\n",
    "            # ln p(x2|x1)\n",
    "            true += model.models[1].log_prob(x[1], x[0])\n",
    "            # ln p(x3|x2)\n",
    "            true += model.models[1].log_prob(x[2], x[1])\n",
    "            # ln p(x4|x3)\n",
    "            true += model.models[1].log_prob(x[3], x[2])\n",
    "            # ln p(x5|x4)\n",
    "            true += model.models[1].log_prob(x[4], x[3])\n",
    "        elif M==2:\n",
    "            # ln p(x1)\n",
    "            true = model.models[0].log_prob(x[0])\n",
    "            # += ln p(x2|x1)\n",
    "            true += model.models[1].log_prob(x[1], x[0])\n",
    "            # += ln p(x3|x2,x1)\n",
    "            true += model.models[2].log_prob(x[2], x[1], x[0])\n",
    "            # += ln p(x4|x3,x2)\n",
    "            true += model.models[2].log_prob(x[3], x[2], x[1])\n",
    "            # += ln p(x5|x4,x3)\n",
    "            true += model.models[2].log_prob(x[4], x[3], x[2])\n",
    "        else:\n",
    "            raise ValueError(f'M = {M} unsupported')\n",
    "            \n",
    "        model_val = model.log_prob(x)\n",
    "        assert np.isclose(true, model_val), 'log joint probability incorrect'\n",
    "        \n",
    "    _test(M=0)\n",
    "    _test(M=1)\n",
    "    _test(M=2)\n",
    "    \n",
    "    \n",
    "test_getset()\n",
    "test_grad_ConditionalDistribution()\n",
    "test_grad_MarkovModel()\n",
    "test_logprob_MarkovModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "associate-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "K, M = 2, 2\n",
    "model = MarkovModel(K=K, M=M)\n",
    "\n",
    "x = np.asarray([0, 0, 1, 0, 0, 1, 0], dtype=int)\n",
    "model.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "extraordinary-crime",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 1, 0, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sample(10, [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "global-confirmation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sample(10, [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "centered-translation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(np.asarray([0, 1, 1]),np.asarray([0, 1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-stopping",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
